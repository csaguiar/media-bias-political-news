{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning text and export training data (Webhose / Google News)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading map for contractions substitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('contractions.json', 'r') as contractions_file:\n",
    "    contractions_dict = json.load(contractions_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset of articles selected from webhose and google news scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../data_processed/selected_articles_webhose_google_news.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper to analyse most common words in each news source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK FOR COMMON WORDS IN EACH NEWS SOURCE TO SPOT ANY REPETITION TO REMOVE FROM ARTICLES\n",
    "# nltk.download('stopwords')\n",
    "# stopwords = nltk.corpus.stopwords.words('english')\n",
    "# def filter_by_domain(dataset, domain):\n",
    "#     return dataset[dataset[\"domain\"] == domain]\n",
    "\n",
    "# def frequence_dist_by_domain(dataset, domain):\n",
    "#     corpus = filter_by_domain(dataset, domain)[\"content\"].sum().lower()\n",
    "#     tokens = tokenizer.tokenize(corpus)\n",
    "#     return nltk.FreqDist(w for w in tokens if w not in stopwords)\n",
    "\n",
    "# for domain in domains:\n",
    "#     print(\"===============================\")\n",
    "#     print(\"DOMAIN {}\".format(domain))\n",
    "#     frequence = frequence_dist_by_domain(dataset, domain)\n",
    "#     print(frequence.most_common(10))\n",
    "#     print(\"===============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to expand contractions\n",
    "def expand_contractions(content):\n",
    "    for original, replacement in contractions_dict.items():\n",
    "        content = content.replace(original, replacement)\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function with rules for each especific domain\n",
    "def clean_by_domain(content, domain):\n",
    "    if domain == \"reuters.com\":\n",
    "        content = re.sub(r\"^(.*?) - \", \"\", content)\n",
    "    \n",
    "    if domain == \"cnn.com\":\n",
    "        content = content.replace(\"(CNN)\", \"\")\n",
    "        \n",
    "    if domain == \"foxnews.com\":\n",
    "        content = re.sub(r\"\\n(.*?) contributed to this report.\", \"\", content)\n",
    "        content = content.replace(\"CLICK HERE TO GET THE FOX NEWS APP\", \"\")\n",
    "    \n",
    "    if domain == \"slate.com\":\n",
    "        content = content.replace(\"Get The Angle in Your Inbox Slate's daily newsletter rounds up the stories you need to read. We encountered an issue signing you up. Please try again. Please enable javascript to use form. Email address: Thanks for signing up! You can manage your newsletter subscriptions at any time.\", \"\")\n",
    "    \n",
    "    if domain == \"bbc.com\":\n",
    "        content = content.replace(\"Image copyright Reuters Image caption\", \"\")\n",
    "        content = content.replace(\"Related Topics\", \"\")\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean a text based on the article entry\n",
    "def clean_text(row):\n",
    "    content = row[\"content\"]\n",
    "    domain = row[\"domain\"]\n",
    "    # Especific cleaning by domain\n",
    "    content = clean_by_domain(content, domain)\n",
    "\n",
    "    # General rules\n",
    "    content = content.replace(\"U.S.\", \"United States\")\n",
    "    content = content.replace(\"U.S\", \"United States\")   \n",
    "    content = content.replace(\"Associated Press\", \"\")\n",
    "    \n",
    "    # lowercase\n",
    "    content = content.lower()\n",
    "    \n",
    "    # Expand contractions\n",
    "    content = expand_contractions(content)\n",
    "    \n",
    "    return ' '.join(tokenizer.tokenize(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned = dataset.copy()\n",
    "dataset_cleaned[\"content\"] = dataset_cleaned.apply(clean_text, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset with labels (removing articles without bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_label = dataset_cleaned[~dataset_cleaned[\"label\"].isna()][[\"content\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5331, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_for_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_label[\"label\"] = dataset_for_label[\"label\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_label.to_csv(\"../data_processed/dataset_with_bias_label.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for bias level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_level = dataset_cleaned[[\"content\", \"level\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6343, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_for_level.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_level.to_csv(\"../data_processed/dataset_with_bias_level.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
